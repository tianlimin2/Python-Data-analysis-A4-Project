# -*- coding: utf-8 -*-
"""Fianl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gpIhBWU8IiHFejwgUmymIaAiPILAW7Yl

The problem consists in classifying all the blocks of the page
   layout of a document that has been detected by a segmentation
   process. This is an essential step in document analysis
   in order to separate text from graphic areas. Indeed,
   the five classes are:<br><b>text (1)<br> horizontal line (2)<br>
   picture (3)<br> vertical line (4)<br> graphic (5)</b>
  
   <b>Relevant Information Paragraph:</b><br
   The 5473 examples comes from 54 distinct documents. <br>
   Each observation concerns one block. <br>
   All attributes are numeric.<br>
   Data are in a format readable by C4.5.<br>

   <b>Number of Instances: 5473.</b>

  <b>Number of Attributes :</b><br>
   <b>height</b>:   integer.         | Height of the block.<br>
   <b>lenght</b>:   integer.     | Length of the block. <br>
   <b>area</b>:     integer.    | Area of the block (height * lenght);<br>
   <b>eccen</b>:    continuous.  | Eccentricity of the block (lenght / height);<br>
   <b>p_black</b>:  continuous.  | Percentage of black pixels within the block (blackpix / area);<br>
   <b>p_and</b>:    continuous.        | Percentage of black pixels after the application of the Run Length Smoothing Algorithm (RLSA) (blackand / area);<br>
   <b>mean_tr</b>:  continuous.      | Mean number of white-black transitions (blackpix / wb_trans);<br>
   <b>blackpix</b>: integer.    | Total number of black pixels in the original bitmap of the block.<br>
   <b>blackand</b>: integer.        | Total number of black pixels in the bitmap of the block after the RLSA.<br>
   <b>wb_trans</b>: integer.          | Number of white-black transitions in the original bitmap of the block.<br>
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns


# columns of dataset
columns = ["height","lenght","area","eccen","p_black","p_and","mean_tr","blackpix","blackand","wb_trans","class"]

data = pd.read_csv("/Users/tianlimin/Downloads/page+blocks+classification/page-blocks.data" ,sep="\s+" ,
                 names=columns,
                 header=None,
                )

data.shape  # dimension of dataset

data.head()

data.info()

data.isna().sum() #no missing value is present

data.describe().T  #statistical analysis

# Basic histogram, looking at the distribution of 'height'
sns.histplot(data['height'], kde=True)
plt.title('Distribution of Block Heights')
plt.xlabel('Height')
plt.show()

data['class'].plot(kind='hist')
plt.title("The distribution of the dataset")
 # highly imbalanced dataset
 
 
# Correlation matrix heatmap between features
plt.figure(figsize=(10, 8))
sns.heatmap(data.corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Feature Correlation Matrix')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# 散点图：展示 'height' 和 'length' 之间的关系
plt.figure(figsize=(8, 6))
plt.scatter(data['height'], data['lenght'])
plt.title('Height vs Length of Blocks')
plt.xlabel('Height')
plt.ylabel('Length')
plt.show()

# 直方图：查看 'area' 的分布
plt.figure(figsize=(8, 6))
plt.hist(data['area'], bins=30)
plt.title('Distribution of Block Area')
plt.xlabel('Area')
plt.ylabel('Frequency')
plt.show()


# 多数直方图：比较 'blackpix' 和 'blackand' 的分布
fig, axes = plt.subplots(1, 2, figsize=(14, 6))
sns.histplot(data['blackpix'], bins=30, ax=axes[0], kde=True)
sns.histplot(data['blackand'], bins=30, ax=axes[1], kde=True)
axes[0].set_title('Distribution of Black Pixels Before RLSA')
axes[1].set_title('Distribution of Black Pixels After RLSA')
axes[0].set_xlabel('Black Pixels (before RLSA)')
axes[1].set_xlabel('Black Pixels (after RLSA)')
axes[0].set_ylabel('Frequency')
axes[1].set_ylabel('Frequency')
plt.show()

"""### Distribution of all attributes"""

i=1
plt.figure(figsize=(20,10))
plt.title("Distribution of all attributes")
for col in data.columns:
    plt.subplot(3,4,i)
    plt.hist(data[col],bins=50)
    plt.tight_layout()
    plt.title(col,fontsize=15)
    i+=1

"""### Relationship between other attributes with target attribute"""

i=1
plt.figure(figsize=(20,10))
plt.title("Relationship between other attributes with target attribute")
for col in data.drop(columns='class').columns:
    plt.subplot(3,4,i)
    plt.scatter(data['class'],data[col])
    plt.tight_layout()
    plt.title(col,fontsize=15)
    i+=1

"""### Boxplot of all attributes ( Outlier Detection )"""

i=1
plt.figure(figsize=(20,10))

for col in data.columns:
    plt.subplot(3,4,i)
    plt.boxplot(data[col])
    plt.tight_layout()
    plt.title(col,fontsize=15)
    i+=1

"""### Removing Outliers"""

df = data[data['height']<250]
df = data[data['area']<35000]
df = data[data['eccen']<300]
df = data[data['mean_tr']<4000]
df = data[data['blackand']<30000]
df = data[data['wb_trans']<2000]

"""### After Removing Outliers"""

i=1
plt.figure(figsize=(20,10))

for col in df.columns:
    plt.subplot(3,4,i)
    plt.boxplot(df[col])
    plt.tight_layout()
    plt.title(col,fontsize=15)
    i+=1

from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report

from sklearn.utils import shuffle
df = shuffle(df)

X = df.drop(columns=['class'])
Y = df['class']

"""#### Normalization ( z-score )"""

X= (X-X.mean())/X.std()

"""### Gaussian NB

#### K-Fold :
 That method is known as “k-fold cross validation”. It’s easy to follow and implement. Below are the steps for it:

1. Randomly split your entire dataset into k”folds”
2. For each k-fold in dataset, build model on k – 1 folds of the dataset. Then, test the model to check the effectiveness for kth fold
3. Record the error on each of the predictions
4. Repeat this until each of the k-folds has served as the test set
5. The average of k recorded errors is called the cross-validation error and will serve as performance metric for the model
"""

GBN = GaussianNB()

scores = pd.DataFrame(columns=['MIN','MAX','AVG']) #dataframe for storing scores

"""#### 5-Fold Cross-validation"""

score5 = cross_val_score(GBN,X,Y,cv=5,verbose=3)

print("MIN - " , score5.min())
print("AVG - " , score5.mean())
print("MAX - " , score5.max())

"""#### 10-Fold Cross-validation"""

score10 = cross_val_score(GBN,X,Y,cv=10,verbose=3)

print("MIN - " , score10.min())
print("AVG - " , score10.mean())
print("MAX - " , score10.max())

"""### Calculate the cross validation score on different split point in k-fold

"""

for i in range(1,20):
    score = cross_val_score(GBN,X,Y,cv=i+1)
    scores.loc[i+1] = [score.min() , score.max() , score.mean()]

plt.figure(figsize=(20,5))

plt.plot(scores['MIN'],marker='o')
plt.plot(scores['MAX'],marker='o')
plt.plot(scores['AVG'],marker='o')

plt.xticks(np.arange(1, 22, 1.0))
plt.legend(['MIN','MAX','AVG'],fontsize=15)
plt.title("Accuracy Vs N_Split" ,fontsize=20)

"""#### Stratified K-Fold
Stratification is the process of rearranging the data so as to ensure that each fold is a good representative of the whole.<br>
It is generally a better approach when dealing with both bias and variance. A randomly selected fold might not adequately represent the minor class, particularly in cases where there is a huge class imbalance.
"""

scores2 = pd.DataFrame(columns=['MIN','MAX','AVG']) #dataframe for storing scores

#calculate the cross validation score on different split point in stratified k-fold and store into dataframe
for i in range(1,20):
    cv = StratifiedKFold(n_splits=i+1,shuffle=True)
    score = cross_val_score(GBN,X,Y,cv=cv)               #Crossvalidation
    scores2.loc[i+1] = [score.min() , score.max() , score.mean()]

plt.figure(figsize=(20,5))

plt.plot(scores2['MIN'],marker='o')
plt.plot(scores2['MAX'],marker='o')
plt.plot(scores2['AVG'],marker='o')

plt.xticks(np.arange(1, 22, 1.0))
plt.legend(['MIN','MAX','AVG'],fontsize=15)
plt.title("Accuracy Vs N_Split" ,fontsize=20)